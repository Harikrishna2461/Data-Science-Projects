{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC-EPEs0wiBI",
        "outputId": "eea2384b-c2c7-49ab-ac74-1d2b3d765b1f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yhGp8QmTuOlp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers, losses, metrics, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from mpl_toolkits import mplot3d\n",
        "import pywt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from PIL import Image\n",
        "\n",
        "import pylab\n",
        "import glob\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import h5py\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "from scipy import signal\n",
        "from scipy.signal import butter, lfilter\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout, Reshape\n",
        "from keras.layers import Bidirectional, BatchNormalization, ZeroPadding1D, Conv2DTranspose\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.callbacks import CSVLogger\n",
        "#from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "#import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import savefig\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_palette('muted')\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score , f1_score , precision_score , recall_score\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import scipy.signal as signal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_data = \"/content/drive/MyDrive/Heart Disease Sound Signal Datasets Updated Folders\"\n",
        "\n",
        "import os\n",
        "img_dirs = []\n",
        "for entry in os.scandir(path_to_data):\n",
        "    if entry.is_dir():\n",
        "        img_dirs.append(entry.path)\n",
        "\n",
        "img_dir_AS = []\n",
        "for entry in os.scandir('/content/drive/MyDrive/Heart Disease Sound Signal Datasets Updated Folders/AS(Aortic Stinosis)'):\n",
        "    img_dir_AS.append(entry.path)\n",
        "\n",
        "img_dir_MR = []\n",
        "for entry in os.scandir('/content/drive/MyDrive/Heart Disease Sound Signal Datasets Updated Folders/MR(Mitral Regurgitation)'):\n",
        "    img_dir_MR.append(entry.path)\n",
        "\n",
        "img_dir_MS = []\n",
        "for entry in os.scandir('/content/drive/MyDrive/Heart Disease Sound Signal Datasets Updated Folders/MS(Mitral Stinosis)'):\n",
        "    img_dir_MS.append(entry.path)\n",
        "\n",
        "img_dir_MVP = []\n",
        "for entry in os.scandir('/content/drive/MyDrive/Heart Disease Sound Signal Datasets Updated Folders/MVP(Mitral Valve Prolapse)'):\n",
        "    img_dir_MVP.append(entry.path)\n",
        "\n",
        "img_dir_N = []\n",
        "for entry in os.scandir('/content/drive/MyDrive/Heart Disease Sound Signal Datasets Updated Folders/N'):\n",
        "    img_dir_N.append(entry.path)"
      ],
      "metadata": {
        "id": "u9pT46hfwkXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STFT Preprocessing**"
      ],
      "metadata": {
        "id": "9v930qpTzw4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_loading_and_preprocessing_stft(img_path,):\n",
        "   signal , sr = librosa.load(img_path, sr=32000, mono=True)\n",
        "   #melspec = librosa.feature.melspectrogram(y=signal,sr=sr)\n",
        "   #melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
        "   stft = librosa.stft(signal)\n",
        "   stft = librosa.amplitude_to_db(np.abs(stft),ref=np.max)\n",
        "   Height = stft.shape[0]\n",
        "   Width = stft.shape[1]\n",
        "   img = Image.fromarray(stft, \"I\")\n",
        "   img.save(\"my.png\")\n",
        "   img = np.array(img)\n",
        "   arr1 = img.reshape((Height,Width,1))\n",
        "   arr2 = np.array(arr1, dtype='uint8')\n",
        "   resized_img = cv2.resize(arr2,(32,32))\n",
        "   resized_img = resized_img.reshape((32,32,1))\n",
        "   stft_rgb = cv2.cvtColor(resized_img,cv2.COLOR_GRAY2RGB)\n",
        "   stft_rgb = np.array(stft_rgb)\n",
        "   return stft_rgb"
      ],
      "metadata": {
        "id": "jT_AdmFoxJHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_stft = [ ]\n",
        "y_stft = [ ]\n",
        "\n",
        "for i in range(len(img_dir_AS)):\n",
        "  image = image_loading_and_preprocessing_stft(img_dir_AS[i])\n",
        "  X_stft.append(image)\n",
        "  y_stft.append(0)\n",
        "\n",
        "for i in range(len(img_dir_MR)):\n",
        "  image = image_loading_and_preprocessing_stft(img_dir_MR[i])\n",
        "  X_stft.append(image)\n",
        "  y_stft.append(1)\n",
        "\n",
        "for i in range(len(img_dir_MS)):\n",
        "  image = image_loading_and_preprocessing_stft(img_dir_MS[i])\n",
        "  X_stft.append(image)\n",
        "  y_stft.append(2)\n",
        "\n",
        "for i in range(len(img_dir_MVP)):\n",
        "  image = image_loading_and_preprocessing_stft(img_dir_MVP[i])\n",
        "  X_stft.append(image)\n",
        "  y_stft.append(3)\n",
        "\n",
        "for i in range(len(img_dir_N)):\n",
        "  image = image_loading_and_preprocessing_stft(img_dir_N[i])\n",
        "  X_stft.append(image)\n",
        "  y_stft.append(4)"
      ],
      "metadata": {
        "id": "uM8jMhPKxQta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_stft = {}\n",
        "dataset_stft['X'] = X_stft\n",
        "dataset_stft['y'] = y_stft\n",
        "dataset_stft = pd.DataFrame(dataset_stft)\n",
        "#Shuffling the Dataset\n",
        "df_stft = dataset_stft.sample(frac = 1)\n",
        "X_shuffled_stft = df_stft['X']\n",
        "y_shuffled_stft = df_stft['y']\n",
        "#Removing the index and converting it into array\n",
        "y_shuffled_stft = list(y_shuffled_stft)\n",
        "y_shuffled_stft = np.array(y_shuffled_stft)\n",
        "X_shuffled_stft = list(X_shuffled_stft)\n",
        "X_shuffled_stft = np.array(X_shuffled_stft)"
      ],
      "metadata": {
        "id": "jqz0PpJ9xYDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the Feature Vector.\n",
        "np.save('Heart_Disease_Prediction_STFT_Feature_Vector',X_shuffled_stft)\n",
        "#Saving the Labels Vector.\n",
        "np.save('Heart_Disease_Prediction_STFT_Label_Vector',y_shuffled_stft)"
      ],
      "metadata": {
        "id": "SCIC4ub4x0oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chirplet Preprocessing**"
      ],
      "metadata": {
        "id": "XGU1s-H_z2g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import chirp"
      ],
      "metadata": {
        "id": "9r95TDlT2pE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_loading_and_preprocessing_chirplet(img_path,):\n",
        "   sample_s , sr = librosa.load(img_path, sr=32000, mono=True)\n",
        "   melspec = librosa.feature.melspectrogram(y=sample_s,sr=sr)\n",
        "   melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
        "   w = chirp(melspec, f0=6, f1=1, t1=10, method='linear')\n",
        "   #w = w.reshape((len(w),1))\n",
        "   #img_cwt = signal.cwt(sample_cwt,signal.ricker,widths)\n",
        "   Height = w.shape[0]\n",
        "   Width = w.shape[1]\n",
        "   img = Image.fromarray(w, \"I\")\n",
        "   img.save(\"my.png\")\n",
        "   img = np.array(img)\n",
        "   arr1 = img.reshape((Height,Width,1))\n",
        "   arr2 = np.array(arr1, dtype='uint8')\n",
        "   resized_img = cv2.resize(arr2,(128,128))\n",
        "   resized_img = resized_img.reshape((128,128,1))\n",
        "   c_rgb_img = cv2.cvtColor(resized_img,cv2.COLOR_GRAY2RGB)\n",
        "   c_rgb_img = np.array(c_rgb_img)\n",
        "   return c_rgb_img"
      ],
      "metadata": {
        "id": "U8mYSEpq4dFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_c = [ ]\n",
        "y_c = [ ]\n",
        "\n",
        "for i in range(len(img_dir_AS)):\n",
        "  image = image_loading_and_preprocessing_chirplet(img_dir_AS[i])\n",
        "  X_c.append(image)\n",
        "  y_c.append(0)\n",
        "\n",
        "for i in range(len(img_dir_MR)):\n",
        "  image = image_loading_and_preprocessing_chirplet(img_dir_MR[i])\n",
        "  X_c.append(image)\n",
        "  y_c.append(1)\n",
        "\n",
        "for i in range(len(img_dir_MS)):\n",
        "  image = image_loading_and_preprocessing_chirplet(img_dir_MS[i])\n",
        "  X_c.append(image)\n",
        "  y_c.append(2)\n",
        "\n",
        "for i in range(len(img_dir_MVP)):\n",
        "  image = image_loading_and_preprocessing_chirplet(img_dir_MVP[i])\n",
        "  X_c.append(image)\n",
        "  y_c.append(3)\n",
        "\n",
        "for i in range(len(img_dir_N)):\n",
        "  image = image_loading_and_preprocessing_chirplet(img_dir_N[i])\n",
        "  X_c.append(image)\n",
        "  y_c.append(4)"
      ],
      "metadata": {
        "id": "U8bBO4-j4gEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_c = {}\n",
        "dataset_c['X'] = X_c\n",
        "dataset_c['y'] = y_c\n",
        "dataset_c = pd.DataFrame(dataset_c)\n",
        "#Shuffling the Dataset\n",
        "df_c = dataset_c.sample(frac = 1)\n",
        "X_shuffled_c = df_c['X']\n",
        "y_shuffled_c = df_c['y']\n",
        "#Removing the index and converting it into array\n",
        "y_shuffled_c = list(y_shuffled_c)\n",
        "y_shuffled_c = np.array(y_shuffled_c)\n",
        "X_shuffled_c = list(X_shuffled_c)\n",
        "X_shuffled_c = np.array(X_shuffled_c)"
      ],
      "metadata": {
        "id": "1O73DTA44oJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the Feature Vector.\n",
        "np.save('Heart_Disease_Prediction_Chirplet_Feature_Vector',X_shuffled_c)\n",
        "#Saving the Labels Vector.\n",
        "np.save('Heart_Disease_Prediction_Chirplet_Label_Vector',y_shuffled_c)"
      ],
      "metadata": {
        "id": "V3GuDxFM4szO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Preprocessed Dataset,use only this code after the first time.**"
      ],
      "metadata": {
        "id": "Mk6Bwics1i9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Feature Vector ('x') and Labels Vector ('y).\n",
        "#STFT\n",
        "X_Loaded_STFT = np.load('/content/Heart_Disease_Prediction_STFT_Feature_Vector.npy')\n",
        "y_Loaded_STFT = np.load('/content/Heart_Disease_Prediction_STFT_Label_Vector.npy')\n",
        "\n",
        "#Chirplet\n",
        "#Loading the Feature Vector ('x') and Labels Vector ('y).\n",
        "X_Loaded_Chirplet = np.load('/content/Heart_Disease_Prediction_Chirplet_Feature_Vector.npy')\n",
        "y_Loaded_Chirplet = np.load('/content/Heart_Disease_Prediction_Chirplet_Label_Vector.npy')"
      ],
      "metadata": {
        "id": "X67GwvqN4zbP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data into train and test\n",
        "X_tr_S,X_test_S,y_tr_S,y_test_S  = train_test_split(X_Loaded_STFT , y_Loaded_STFT , test_size=0.2 , random_state=42)\n",
        "X_train_S,X_val_S,y_train_S,y_val_S  = train_test_split(X_tr_S , y_tr_S , test_size=0.2 , random_state=42)"
      ],
      "metadata": {
        "id": "Et4QQSm83evR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_C,X_test_C,y_tr_C,y_test_C  = train_test_split(X_Loaded_STFT , y_Loaded_STFT , test_size=0.2 , random_state=42)\n",
        "X_train_C,X_val_C,y_train_C,y_val_C  = train_test_split(X_tr_C , y_tr_C , test_size=0.2 , random_state=42)"
      ],
      "metadata": {
        "id": "ygYNed6J2F-F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_C.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAiwvFGZ6fEG",
        "outputId": "3d31b8ae-f4b1-45cd-d81e-b70d09067ee3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(640, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_S.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czxg4I6e6hhg",
        "outputId": "86ca42e9-d79b-4eb3-a36b-ffd9b3c75d3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(640, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stft_model():\n",
        "    model = tf.keras.Sequential(\n",
        "           [\n",
        "             tf.keras.layers.InputLayer(input_shape = (32,32,3)),\n",
        "             tf.keras.layers.Conv2D(60, kernel_size = (3,3),activation = 'relu',input_shape=(32,32,3)),\n",
        "             tf.keras.layers.MaxPooling2D((2,2)),\n",
        "             tf.keras.layers.BatchNormalization(),\n",
        "             tf.keras.layers.Dense(128,activation = 'relu'),\n",
        "             tf.keras.layers.Dropout(0.2),\n",
        "             tf.keras.layers.Dense(64,activation = 'relu'),\n",
        "             tf.keras.layers.Dense(32,activation = 'relu'),\n",
        "             tf.keras.layers.Flatten(),\n",
        "             tf.keras.layers.Dense(5,activation = 'softmax')\n",
        "           ]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "IIC6ws1i5ZSE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chirplet_model():\n",
        "    model = tf.keras.Sequential(\n",
        "           [\n",
        "             tf.keras.layers.Input((32, 32, 3)),\n",
        "             tf.keras.layers.Conv2D(60, kernel_size = (3,3),activation = 'relu',input_shape=(32,32,3)),\n",
        "             tf.keras.layers.MaxPooling2D((2,2)),\n",
        "             tf.keras.layers.BatchNormalization(),\n",
        "             tf.keras.layers.Dense(128,activation = 'relu'),\n",
        "             tf.keras.layers.Dropout(0.2),\n",
        "             tf.keras.layers.Dense(64,activation = 'relu'),\n",
        "             tf.keras.layers.Dense(32,activation = 'relu'),\n",
        "             tf.keras.layers.Flatten(),\n",
        "             tf.keras.layers.Dense(5,activation = 'softmax')\n",
        "\n",
        "           ]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "iJEsO4B7651J"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chirplet_model = chirplet_model()\n",
        "stft_model = stft_model()"
      ],
      "metadata": {
        "id": "l7ERyGpqRM79"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chirplet_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF2bGijuZEPj",
        "outputId": "6b2c777a-d24d-46b9-bb4d-919b9fb1afe1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 30, 30, 60)        1680      \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPoolin  (None, 15, 15, 60)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 15, 15, 60)        240       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 15, 15, 128)       7808      \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 15, 15, 128)       0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 15, 15, 64)        8256      \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 15, 15, 32)        2080      \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 7200)              0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 5)                 36005     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 56069 (219.02 KB)\n",
            "Trainable params: 55949 (218.55 KB)\n",
            "Non-trainable params: 120 (480.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling and Training the Chirplet Model\n",
        "chirplet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss='sparse_categorical_crossentropy',metrics='accuracy')\n",
        "chirplet_model.fit(X_train_C, y_train_C, epochs=10,validation_data = (X_val_C,y_val_C))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz8HlqZYULpb",
        "outputId": "66393017-6f60-4ec6-e378-f5a77ba239cd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 4s 110ms/step - loss: 1.1537 - accuracy: 0.5359 - val_loss: 0.7628 - val_accuracy: 0.8125\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 2s 77ms/step - loss: 0.4846 - accuracy: 0.8250 - val_loss: 0.6280 - val_accuracy: 0.8562\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 1s 75ms/step - loss: 0.3110 - accuracy: 0.9000 - val_loss: 0.4676 - val_accuracy: 0.8625\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.2217 - accuracy: 0.9125 - val_loss: 0.4831 - val_accuracy: 0.8875\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 2s 76ms/step - loss: 0.1495 - accuracy: 0.9563 - val_loss: 0.3918 - val_accuracy: 0.8938\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 2s 82ms/step - loss: 0.0937 - accuracy: 0.9750 - val_loss: 0.3955 - val_accuracy: 0.8813\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 2s 120ms/step - loss: 0.0571 - accuracy: 0.9875 - val_loss: 0.4730 - val_accuracy: 0.8813\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 4s 177ms/step - loss: 0.0567 - accuracy: 0.9859 - val_loss: 0.3995 - val_accuracy: 0.8938\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 2s 78ms/step - loss: 0.0284 - accuracy: 0.9984 - val_loss: 0.4434 - val_accuracy: 0.8938\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 0.0161 - accuracy: 0.9969 - val_loss: 0.4841 - val_accuracy: 0.8813\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7effe3b31090>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling and Training the STFT Model\n",
        "stft_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss='sparse_categorical_crossentropy',metrics='accuracy')\n",
        "stft_model.fit(X_train_S, y_train_S, epochs=10,validation_data = (X_val_S,y_val_S))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxtQhfXeU1dg",
        "outputId": "ded9275c-fb49-4d04-e5bf-0b23742e45e2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 8s 91ms/step - loss: 1.0109 - accuracy: 0.5953 - val_loss: 1.0439 - val_accuracy: 0.6625\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 2s 111ms/step - loss: 0.4533 - accuracy: 0.8219 - val_loss: 0.5470 - val_accuracy: 0.8500\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 2s 83ms/step - loss: 0.3174 - accuracy: 0.8922 - val_loss: 0.4895 - val_accuracy: 0.8625\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 2s 76ms/step - loss: 0.2039 - accuracy: 0.9297 - val_loss: 0.4517 - val_accuracy: 0.8750\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 2s 108ms/step - loss: 0.1315 - accuracy: 0.9578 - val_loss: 0.3757 - val_accuracy: 0.9125\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0963 - accuracy: 0.9734 - val_loss: 0.3905 - val_accuracy: 0.8875\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 2s 98ms/step - loss: 0.0885 - accuracy: 0.9719 - val_loss: 0.4229 - val_accuracy: 0.9000\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 2s 80ms/step - loss: 0.0562 - accuracy: 0.9828 - val_loss: 0.5607 - val_accuracy: 0.8875\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 2s 80ms/step - loss: 0.0428 - accuracy: 0.9875 - val_loss: 0.4331 - val_accuracy: 0.9062\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 1s 71ms/step - loss: 0.0280 - accuracy: 0.9937 - val_loss: 0.4020 - val_accuracy: 0.9000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7effe4e7e6e0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Extract features or predictions from the final layers of both models\n",
        "chirplet_features = chirplet_model.predict(X_train_C)\n",
        "stft_features = stft_model.predict(X_train_S)\n",
        "\n",
        "# Concatenate or combine the features\n",
        "combined_features = np.concatenate((X_train_C, X_train_S), axis=1)\n",
        "combined_labels = np.concatenate((y_train_C , y_train_S), axis=0)\n",
        "\n",
        "combined_validation_features = np.concatenate((X_val_C, X_val_S), axis=1)\n",
        "combined_validation_labels = np.concatenate((y_val_C , y_val_S), axis=0)\n",
        "\n",
        "# Define and train a fully connected network on the combined features\n",
        "def create_fully_connected_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(combined_features.shape[1],)),\n",
        "        tf.keras.layers.Dense(64,activation='relu'),\n",
        "        tf.keras.layers.Dense(32,activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')  # Adjust num_classes based on your task\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "fully_connected_model = create_fully_connected_model()\n",
        "\n",
        "# Train the fully connected model\n",
        "fully_connected_model.fit(combined_features, combined_labels, epochs=10)  # Replace labels\n",
        "\n",
        "# Evaluate the ensemble model on validation or test data\n",
        "evaluation_result = fully_connected_model.evaluate(combined_validation_features, combined_validation_features)  # Replace validation_data and validation_labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "QiYtndRPQ-hc",
        "outputId": "8de352d2-b85a-4ca0-b481-51867df826f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ac2919cce7b8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train chirplet and STFT models on their respective datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchirplet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace chirplet_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstft_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace stft_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Extract features or predictions from the final layers of both models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3873\u001b[0m         \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3874\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3875\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   3876\u001b[0m                 \u001b[0;34m\"You must compile your model before \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3877\u001b[0m                 \u001b[0;34m\"training/testing. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the ensemble model on new data\n",
        "predictions = fully_connected_model.predict(new_data)"
      ],
      "metadata": {
        "id": "arsOPFIdT9x2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}